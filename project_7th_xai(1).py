# -*- coding: utf-8 -*-
"""Project 7th XAI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EObgb3aUezqy7hd6kt8kso0xgkXpEa4L

##Import the libraries
"""

# 1. Install packages not included in standard Colab
# !pip install -q kaggle lime shap scikit-image

# 2. Standard Data Science Libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import cv2
from PIL import Image

# 3. Machine Learning (Scikit-Learn)
from sklearn.model_selection import train_test_split , cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, mean_squared_error, r2_score ,classification_report , confusion_matrix, f1_score
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
from imblearn.over_sampling import SMOTE


# 4. Models
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor
from sklearn.svm import SVC, SVR
from sklearn.neural_network import MLPClassifier, MLPRegressor

# 5. Explainable AI (XAI) & Inspection
from sklearn.inspection import PartialDependenceDisplay, permutation_importance
!pip install -q kaggle lime shap scikit-image
import lime
from lime import lime_tabular, lime_image, lime_text
from sklearn.utils import resample # Needed for ICE sampling

# 6. Text & Image Specific
from sklearn.feature_extraction.text import TfidfVectorizer
from skimage.io import imread
from skimage.transform import resize
from skimage.color import rgb2gray

# 7. Settings
import warnings
warnings.filterwarnings('ignore')
sns.set_theme(style="whitegrid")

from glob import glob
from tqdm import tqdm
from scipy.sparse import hstack

print("Libraries loaded successfully!")

# 1. Install the Kaggle library
!pip install -q kaggle

# 2. Upload your kaggle.json file
# ACTION REQUIRED: Run this cell, click "Choose Files", and select your kaggle.json file.
from google.colab import files
print("Please upload your kaggle.json file:")
files.upload()

# 3. Move the file to the correct directory and set permissions
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
print("Kaggle API setup complete!")

"""#Dataset 1 : Airline Passenger Satisfaction"""

print("\nDownloading Dataset 1 :")
!mkdir -p dataset_1
!kaggle datasets download -d teejmahal20/airline-passenger-satisfaction -p dataset_1

!unzip -q dataset_1/airline-passenger-satisfaction.zip -d dataset_1
!rm dataset_1/airline-passenger-satisfaction.zip
print("Dataset 1 ready in folder: dataset_1/")

"""## Analyze dataset"""

df = pd.read_csv('dataset_1/train.csv')

df.shape

df.head()

df.columns

df.info()

df.duplicated().sum()

numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns
numeric_cols

categorical_cols = df.select_dtypes(include=['object']).columns
categorical_cols

df[numeric_cols].describe()

df[categorical_cols].describe()

# 7️⃣ Distribution of target variable
plt.figure(figsize=(6,4))
sns.countplot(data=df, x='satisfaction')
plt.title('Distribution of satisfaction')
plt.show()

# 2. Convert 'Satisfaction' to numeric (if it's text)
# Adjust the mapping {'satisfied': 1, 'neutral or dissatisfied': 0} based on your actual data values
if 'Satisfaction' in df.columns and df['Satisfaction'].dtype == 'object':
    df['Satisfaction_Binary'] = df['Satisfaction'].map({'satisfied': 1, 'neutral or dissatisfied': 0})

# 3. Calculate Correlations (Numeric columns only)
numeric_df = df.select_dtypes(include=['number'])
corr_matrix = numeric_df.corr()

# 4. Plot Heatmap
plt.figure(figsize=(14, 10))
sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

# 2. Setup the figure size
fig, axes = plt.subplots(2, 2, figsize=(14, 10))
fig.suptitle('Histogram Analysis', fontsize=16)

# 3. Plot Histograms
# Change 'Age', 'Flight Distance', etc. to match your exact column names

# Age
sns.histplot(data=df, x='Age', kde=True, bins=20, color='skyblue', ax=axes[0, 0])
axes[0, 0].set_title('Age Distribution')

# Flight Distance
sns.histplot(data=df, x='Flight Distance', kde=True, bins=30, color='salmon', ax=axes[0, 1])
axes[0, 1].set_title('Flight Distance Distribution')

# Wifi Service (Categorical/Ordinal)
sns.countplot(data=df, x='Inflight wifi service', color='lightgreen', ax=axes[1, 0])
axes[1, 0].set_title('Wifi Rating Distribution')

# Departure Delay (limit x-axis to see better if data is skewed)
sns.histplot(data=df, x='Departure Delay in Minutes', kde=True, bins=50, color='orange', ax=axes[1, 1])
axes[1, 1].set_title('Departure Delay Distribution')
axes[1, 1].set_xlim(0, 150) # Optional: zoom in on 0-150 minutes

plt.tight_layout()
plt.show()

"""##Feature engineering"""

cols_to_drop = ['id', 'Unnamed: 0']
df = df.drop(columns=[col for col in cols_to_drop if col in df.columns])

# Impute missing 'Arrival Delay'
# Strategy: If Arrival Delay is missing, assume it's similar to Departure Delay or 0
df['Arrival Delay in Minutes'] = df['Arrival Delay in Minutes'].fillna(df['Departure Delay in Minutes'])

# A. Create "Total Delay"
# Instead of two separate correlated columns, combine them to capture the total time lost.
df['Total_Delay'] = df['Departure Delay in Minutes'] + df['Arrival Delay in Minutes']

# B. Log Transformation for Skewed Data
# Your histogram showed Delays are highly right-skewed. Log transform helps normalize this.
# We add +1 to avoid log(0) errors.
df['Total_Delay_Log'] = np.log1p(df['Total_Delay'])

# C. Binning Flight Distance (Optional but helpful for some models)
# Categorize into Short, Medium, Long Haul
df['Flight_Range'] = pd.cut(df['Flight Distance'],
                            bins=[0, 1500, 3000, 10000],
                            labels=['Short', 'Medium', 'Long'])

# List of columns containing service ratings (0-5)
service_cols = ['Inflight wifi service', 'Departure/Arrival time convenient',
                'Ease of Online booking', 'Gate location', 'Food and drink',
                'Online boarding', 'Seat comfort', 'Inflight entertainment',
                'On-board service', 'Leg room service', 'Baggage handling',
                'Checkin service', 'Inflight service', 'Cleanliness']

# Create an "Overall Experience Score"
# Averaging these gives a single metric for "Luxury/Comfort" level
df['Average_Service_Rating'] = df[service_cols].mean(axis=1)

# A. Label Encoding for Target Variable
le = LabelEncoder()
df['satisfaction'] = le.fit_transform(df['satisfaction'])
# 0 = neutral or dissatisfied, 1 = satisfied (usually)

# B. One-Hot Encoding for Nominal Variables
# Columns: Gender, Customer Type, Type of Travel, Class, Flight_Range (created above)
categorical_features = ['Gender', 'Customer Type', 'Type of Travel', 'Class', 'Flight_Range']
df_encoded = pd.get_dummies(df, columns=categorical_features, drop_first=True)

scaler = StandardScaler()
continuous_cols = ['Age', 'Flight Distance', 'Total_Delay', 'Average_Service_Rating']

df_encoded[continuous_cols] = scaler.fit_transform(df_encoded[continuous_cols])

# Final Check
print("New Data Shape:", df_encoded.shape)

df_encoded.head()

"""## Train and evaluate for the model."""

# Define Features (X) and Target (y)
# 'satisfaction' is our target (0 or 1 from LabelEncoder)
X = df_encoded.drop('satisfaction', axis=1)
y = df_encoded['satisfaction']

# Split into Train (80%) and Test (20%) sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"Training set shape: {X_train.shape}")
print(f"Testing set shape: {X_test.shape}")

models = {
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "Gradient Boosting": GradientBoostingClassifier(random_state=42),
    "Multi-Layer Perceptron (MLP)": MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)
}

results = {}
print("\n--- Model Training Log ---")

for name, model in models.items():
    print(f"Training {name}...")
    # A. Train the model
    model.fit(X_train, y_train)
    # B. Make predictions
    y_pred = model.predict(X_test)
    # C. Calculate Accuracy
    acc = accuracy_score(y_test, y_pred)
    results[name] = acc
    print(f"--> {name} Accuracy: {acc:.4f}")

    # Optional: Print detailed report for the best performers (usually RF or GB)
    if name in ["Random Forest", "Gradient Boosting"]:
        print(classification_report(y_test, y_pred))
        print("-" * 30)

# Print summary table
results_df = pd.DataFrame(list(results.items()), columns=['Model', 'Accuracy']).sort_values(by='Accuracy', ascending=False)
print("\nFinal Leaderboard:")
print(results_df)

"""##Apply the XAI"""

print("Computing Permutation Feature Importance...")
result = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1)

# Sort features by importance
sorted_idx = result.importances_mean.argsort()[::-1]

# Plot Top 10 Features
plt.figure(figsize=(10, 6))
plt.boxplot(result.importances[sorted_idx].T, vert=False, labels=X_test.columns[sorted_idx])
plt.title("Permutation Feature Importance (Test Set)")
plt.axvline(x=0, color="k", linestyle="--")
plt.xlabel("Decrease in Accuracy Score")
plt.tight_layout()
plt.show()

print(model)

print("Generating LIME explanation...")

# Initialize LIME Explainer
explainer = lime.lime_tabular.LimeTabularExplainer(
    training_data=np.array(X_train),
    feature_names=X_train.columns,
    class_names=['Neutral/Dissatisfied', 'Satisfied'],
    mode='classification'
)

# Pick a random passenger from the test set to explain (e.g., index 10)
idx = 10
passenger_data = X_test.iloc[idx]
true_label = y_test.iloc[idx]

print(f"Explaining Passenger #{idx} | True Label: {true_label}")

# Generate explanation
exp = explainer.explain_instance(
    data_row=passenger_data,
    predict_fn=model.predict_proba
)

# Visualize
exp.as_pyplot_figure()
plt.title(f"LIME Explanation for Passenger #{idx}")
plt.tight_layout()
plt.show()

target_feature = 'Average_Service_Rating'

print(f"Generating PDP for: {target_feature}...")

# 2. Create the Plot
fig, ax = plt.subplots(figsize=(10, 6))

# We use a sample of 500 points to make it run fast and avoid memory errors
X_sample = X_test.sample(n=500, random_state=42)

display = PartialDependenceDisplay.from_estimator(
    estimator=model,
    X=X_sample,
    features=[target_feature],
    kind='average', # <--- 'average' = PDP Only (No error-prone ICE lines)
    ax=ax
)

plt.title(f"Partial Dependence Plot: {target_feature}")
plt.grid(True, alpha=0.3)
plt.show()

target_feature = 'Age'

print(f"Generating ICE Plot for: {target_feature}...")

# 2. Sample the data
# We only plot 50 random passengers to keep the visualization clean
X_sample = X_test.sample(n=100, random_state=42)

fig, ax = plt.subplots(figsize=(10, 6))

# 3. Generate ICE Plot
display = PartialDependenceDisplay.from_estimator(
    estimator=model,
    X=X_sample,
    features=[target_feature],
    kind='individual',  # <--- 'individual' = ICE Only
    ax=ax
)

# 4. Styling for better visibility
# Access the lines to make them thinner and semi-transparent
for lines in display.lines_[0, 0]:
    lines.set_alpha(0.5)
    lines.set_linewidth(1.5)
    lines.set_color('#1f77b4') # Standard Matplotlib Blue

plt.title(f"ICE Plot: How '{target_feature}' affects Satisfaction for 50 Individuals")
plt.grid(True, alpha=0.3)
plt.show()

"""#Dataste 2 : California Housing Prices"""

print("\nDownloading Dataset 2 :")
!mkdir -p dataset_2
!kaggle datasets download -d camnugent/california-housing-prices -p dataset_2

!unzip -q dataset_2/california-housing-prices.zip -d dataset_2
!rm dataset_2/california-housing-prices.zip
print("Dataset 2 ready in folder: dataset_2/")

"""## Analyze dataset"""

df = pd.read_csv('dataset_2/housing.csv')

df.shape

df.head()

df.columns

df.info()

df.duplicated().sum()

numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns
numeric_cols

categorical_cols = df.select_dtypes(include=['object']).columns
categorical_cols

df[numeric_cols].describe()

df[categorical_cols].describe()

plt.figure(figsize=(6,4))
sns.histplot(data=df, x='median_house_value', kde=True, bins=50)
plt.title('Distribution of Median House Value')
plt.xlabel('Median House Value')
plt.ylabel('Frequency')
plt.show()

# 2. Convert 'Satisfaction' to numeric (if it's text)
# Adjust the mapping {'satisfied': 1, 'neutral or dissatisfied': 0} based on your actual data values
if 'Satisfaction' in df.columns and df['Satisfaction'].dtype == 'object':
    df['Satisfaction_Binary'] = df['Satisfaction'].map({'satisfied': 1, 'neutral or dissatisfied': 0})

# 3. Calculate Correlations (Numeric columns only)
numeric_df = df.select_dtypes(include=['number'])
corr_matrix = numeric_df.corr()

# 4. Plot Heatmap
plt.figure(figsize=(14, 10))
sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

# 2. Setup the figure size
fig, axes = plt.subplots(2, 2, figsize=(14, 10))
fig.suptitle('Histogram Analysis of Housing Data', fontsize=16)

# 3. Plot Histograms for relevant columns from the current dataset

# Housing Median Age
sns.histplot(data=df, x='housing_median_age', kde=True, bins=20, color='skyblue', ax=axes[0, 0])
axes[0, 0].set_title('Housing Median Age Distribution')

# Total Rooms
sns.histplot(data=df, x='total_rooms', kde=True, bins=30, color='salmon', ax=axes[0, 1])
axes[0, 1].set_title('Total Rooms Distribution')

# Population
sns.histplot(data=df, x='population', kde=True, bins=50, color='lightgreen', ax=axes[1, 0])
axes[1, 0].set_title('Population Distribution')

# Median Income
sns.histplot(data=df, x='median_income', kde=True, bins=50, color='orange', ax=axes[1, 1])
axes[1, 1].set_title('Median Income Distribution')

plt.tight_layout()
plt.show()

"""##Feature engineering"""

# Separate features (X) and target (y)
X = df.drop("median_house_value", axis=1)
y = df["median_house_value"]

# Define the pipeline for Numeric Features
# 1. Impute missing values with median
# 2. Scale features using StandardScaler
num_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy="median")),
    ('std_scaler', StandardScaler()),
])

# Define the pipeline for Categorical Features
# 1. One-hot encode 'ocean_proximity'
cat_pipeline = Pipeline([
    ('one_hot', OneHotEncoder(handle_unknown='ignore')),
])

# Combine into a full preprocessor
full_pipeline = ColumnTransformer([
    ("num", num_pipeline, X.select_dtypes(include=['float64', 'int64']).columns),
    ("cat", cat_pipeline, ["ocean_proximity"]),
])

# Prepare the data
X_prepared = full_pipeline.fit_transform(X)

# Get feature names back for XAI later
num_features = list(X.select_dtypes(include=['float64', 'int64']).columns)
cat_encoder = full_pipeline.named_transformers_['cat']['one_hot']
cat_features = list(cat_encoder.get_feature_names_out(['ocean_proximity']))
feature_names = num_features + cat_features

# Split into Train and Test sets
X_train, X_test, y_train, y_test = train_test_split(X_prepared, y, test_size=0.2, random_state=42)

print(f"Training shape: {X_train.shape}")
print(f"Testing shape: {X_test.shape}")

"""##Train and evaluate for the model.

"""

models = {
    "SVM": SVR(kernel="rbf", C=100, gamma=0.1, epsilon=0.1), # Adjusted for regression
    "MLP": MLPRegressor(hidden_layer_sizes=[50, 50], max_iter=500, random_state=42),
    "Random Forest": RandomForestRegressor(n_estimators=100, random_state=42),
    "Gradient Boosting": GradientBoostingRegressor(n_estimators=100, random_state=42)
}

# Dictionary to store trained models for XAI
trained_models = {}

# Loop to train and evaluate
for name, model in models.items():
    print(f"Training {name}...")
    model.fit(X_train, y_train)
    trained_models[name] = model

    # Predict
    y_pred = model.predict(X_test)

    # Evaluate
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    r2 = r2_score(y_test, y_pred)

    print(f"  -> RMSE: {rmse:,.2f}")
    print(f"  -> R2 Score: {r2:.4f}\n")

"""##Apply the XAI"""

from sklearn.inspection import permutation_importance

# Select the best model (e.g., Random Forest)
best_model = trained_models["Random Forest"]

result = permutation_importance(best_model, X_test, y_test, n_repeats=10, random_state=42)

# Sort and visualize
sorted_idx = result.importances_mean.argsort()

plt.figure(figsize=(10, 6))
plt.boxplot(result.importances[sorted_idx].T, vert=False, labels=[feature_names[i] for i in sorted_idx])
plt.title("Permutation Importances (Test Set)")
plt.tight_layout()
plt.show()

from sklearn.inspection import PartialDependenceDisplay

# Features to analyze (e.g., Median Income and House Age)
features_to_plot = ['median_income', 'housing_median_age', ('median_income', 'housing_median_age')]
# Note: We need the indices of these features in the transformed matrix
indices = [feature_names.index(f) if f in feature_names else 0 for f in ['median_income', 'housing_median_age']]

print("Generating Partial Dependence Plots...")
PartialDependenceDisplay.from_estimator(best_model, X_test[:100], [indices[0], indices[1]], feature_names=feature_names)
plt.suptitle("Partial Dependence Plots")
plt.show()

print("Generating ICE Plots...")
# Plot ICE for Median Income
PartialDependenceDisplay.from_estimator(
    best_model,
    X_test[:50],  # Use a subset for clarity
    [indices[0]],
    feature_names=feature_names,
    kind="both"   # 'both' plots individual lines (ICE) and the average (PDP)
)
plt.title("ICE Plot for Median Income")
plt.show()

# You may need to install lime: !pip install lime
import lime
import lime.lime_tabular

# Initialize LIME explainer
explainer = lime.lime_tabular.LimeTabularExplainer(
    training_data=X_train,
    feature_names=feature_names,
    class_names=['median_house_value'],
    mode='regression'
)

# Pick a specific instance to explain (e.g., the 5th test instance)
i = 5
exp = explainer.explain_instance(X_test[i], best_model.predict, num_features=5)

# Show the explanation
# In a notebook, you would use exp.show_in_notebook(show_table=True)
print(f"\nActual Value: {y_test.iloc[i]}")
print(f"Predicted Value: {best_model.predict(X_test[i].reshape(1, -1))[0]}")
print("\nLIME Explanation (Feature Contributions):")
for feature, weight in exp.as_list():
    print(f"{feature}: {weight:.2f}")

# To visualize
exp.as_pyplot_figure()
plt.tight_layout()
plt.show()

"""#Dataset 3 : SMS Spam Collection Dataset"""

print("\nDownloading Dataset 3 :")
!mkdir -p dataset_3
!kaggle datasets download -d uciml/sms-spam-collection-dataset -p dataset_3

!unzip -q dataset_3/sms-spam-collection-dataset.zip -d dataset_3
!rm dataset_3/sms-spam-collection-dataset.zip
print("Dataset 3 ready in folder: dataset_3/")

"""## Analyze dataset"""

df = pd.read_csv('dataset_3/spam.csv', encoding='latin1')

df.shape

df.head()

df.columns

df.info()

df.duplicated().sum()

numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns
numeric_cols

categorical_cols = df.select_dtypes(include=['object']).columns
categorical_cols

df[categorical_cols].describe()

plt.figure(figsize=(6,4))
sns.countplot(data=df, x='v1', palette='viridis')
plt.title('Distribution of Ham vs. Spam Messages')
plt.xlabel('Message Type')
plt.ylabel('Count')
plt.show()

# 3. Duplicates
print(f"\nDuplicates: {df.duplicated().sum()}")
df = df.drop_duplicates(keep='first') # Important for text data
print(f"New Shape after dropping duplicates: {df.shape}")

# 4. Target Distribution
print("\nClass Distribution:")
print(df['v1'].value_counts()) # Changed 'label' to 'v1'

# 5. Feature Engineering for Analysis (Meta-features)
df['length'] = df['v2'].apply(len) # Changed 'message' to 'v2'

"""##Feature engineering"""

# 1. Encode Target (Ham=0, Spam=1)
le = LabelEncoder()
y = le.fit_transform(df['v1'])

# 2. Split Data
X_raw = df['v2']
X_train_raw, X_test_raw, y_train, y_test = train_test_split(X_raw, y, test_size=0.2, random_state=42)

# 3. Vectorization (The Feature Engineering Step)
# We limit to top 3000 words to keep models fast and interpretable
tfidf = TfidfVectorizer(stop_words='english', max_features=3000)

X_train = tfidf.fit_transform(X_train_raw).toarray()
X_test = tfidf.transform(X_test_raw).toarray()

feature_names = tfidf.get_feature_names_out()

print(f"Training Matrix Shape: {X_train.shape}")

"""## Train and evaluate for the model."""

# Initialize Models
models = {
    "SVM": SVC(kernel='linear', probability=True, random_state=42), # Linear kernel is often best for text
    "MLP": MLPClassifier(hidden_layer_sizes=(50,), max_iter=300, random_state=42),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "Gradient Boosting": GradientBoostingClassifier(n_estimators=100, random_state=42)
}

trained_models = {}

for name, model in models.items():
    print(f"Training {name}...")
    model.fit(X_train, y_train)
    trained_models[name] = model

    # Predict
    y_pred = model.predict(X_test)

    # Evaluate
    acc = accuracy_score(y_test, y_pred)
    print(f"  -> Accuracy: {acc:.4f}")

"""## Apply the XAI"""

# SVM Coefficients (Linear Kernel) allow direct interpretation
if "SVM" in trained_models:
    coefs = trained_models["SVM"].coef_.flatten()

    # Sort by importance
    top_positive = np.argsort(coefs)[-10:] # Most Spam-like words

    plt.figure(figsize=(10, 5))
    colors = ['red' if coefs[i] > 0 else 'blue' for i in top_positive]
    plt.barh([feature_names[i] for i in top_positive], coefs[top_positive], color=colors)
    plt.title("Top 10 Words identifying SPAM (SVM Coefficients)")
    plt.show()

from lime.lime_text import LimeTextExplainer
from sklearn.pipeline import make_pipeline

print("\n--- Starting LIME Analysis ---")

# 1. Select the Best Model
# We use SVM because you set probability=True, which LIME requires.
best_model = trained_models["SVM"]

# 2. Create the Pipeline
# This is crucial: LIME generates raw text. We need a pipeline that:
# Raw Text -> TFIDF -> Dense Array -> SVM Prediction
def predict_proba_fn(texts):
    # Transform text to vector
    features = tfidf.transform(texts)
    # Convert to dense array (because you trained the model on dense arrays)
    features_dense = features.toarray()
    return best_model.predict_proba(features_dense)

# 3. Initialize Explainer
explainer = LimeTextExplainer(class_names=['Ham', 'Spam'])

# 4. Pick a specific SPAM message to explain
# We use 'y_test' to find a spam index, but we grab the text from 'X_test_raw'
spam_indices = np.where(y_test == 1)[0]
idx = spam_indices[0] # Pick the first spam message in the test set

# Get the actual raw text string
text_instance = X_test_raw.iloc[idx]
true_label = le.inverse_transform([y_test[idx]])[0]

print(f"Message Index: {idx}")
print(f"Message Content: {text_instance}")
print(f"Actual Label: {true_label}")

# 5. Generate Explanation
# We pass the raw text and our custom prediction function
exp = explainer.explain_instance(
    text_instance,
    predict_proba_fn,
    num_features=6
)

# 6. Visualize Results
print("\nLIME Explanation (Word Contributions):")
# Positive values = Contributes to SPAM, Negative = Contributes to HAM
for word, weight in exp.as_list():
    print(f"{word:<15}: {weight:.4f}")

# Show Plot

fig = exp.as_pyplot_figure()
plt.tight_layout()
plt.show()

"""#Dataset 4 : Face Mask Detection"""

!pip install kagglehub
import kagglehub
import os
path = kagglehub.dataset_download("ashishjangra27/face-mask-12k-images-dataset")

print("Path to dataset files:", path)

dataset_path = os.path.join(path, 'Face Mask Dataset')
IMG_SIZE = 64
categories = ['WithoutMask', 'WithMask']

print("--- Loading Training Data ---")
train_path = os.path.join(dataset_path, 'Train')
X_train_data = []
y_train_data = []

if os.path.exists(train_path):
    for label_int, category in enumerate(categories):
        folder_path = os.path.join(train_path, category)
        # Limit to first 1000 per class for speed if needed, or load all
        for img_name in os.listdir(folder_path):
            try:
                img_path = os.path.join(folder_path, img_name)
                img_array = cv2.imread(img_path)
                img_array = cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB)
                new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))
                X_train_data.append(new_array)
                y_train_data.append(label_int)
            except Exception as e:
                pass
else:
    print("Train folder not found!")

X_train = np.array(X_train_data)
y_train = np.array(y_train_data)
print(f"Training loaded: {X_train.shape}")

print("--- Loading Test Data ---")
test_path = os.path.join(dataset_path, 'Test')
X_test_data = []
y_test_data = []

if os.path.exists(test_path):
    for label_int, category in enumerate(categories):
        folder_path = os.path.join(test_path, category)
        for img_name in os.listdir(folder_path):
            try:
                img_path = os.path.join(folder_path, img_name)
                img_array = cv2.imread(img_path)
                img_array = cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB)
                new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))
                X_test_data.append(new_array)
                y_test_data.append(label_int)
            except Exception as e:
                pass
else:
    print("Test folder not found!")

X_test = np.array(X_test_data)
y_test = np.array(y_test_data)
print(f"Testing loaded: {X_test.shape}")

# Show simple samples
fig, axes = plt.subplots(1, 4, figsize=(15, 5))
for i in range(4):
    idx = np.random.randint(0, len(X_train_data))
    axes[i].imshow(X_train_data[idx])
    axes[i].set_title(f"Label: {'With Mask' if y_train[idx] == 1 else 'Without Mask'}")
    axes[i].axis('off')
plt.suptitle("Step 1: Simple Dataset Visualization")
plt.show()

"""##Feature engineering"""

X_train_norm = X_train / 255.0
X_test_norm = X_test / 255.0

# Flatten for MLP (N, 64, 64, 3) -> (N, 12288)
X_train_flat = X_train_norm.reshape(X_train_norm.shape[0], -1)
X_test_flat = X_test_norm.reshape(X_test_norm.shape[0], -1)

"""## Train and evaluate for the model."""

print("\n--- Training MLP ---")
# Using MLPClassifier
mlp = MLPClassifier(
    hidden_layer_sizes=(128, 64),
    activation='relu',
    solver='adam',
    max_iter=15,
    random_state=42,
    verbose=True
)

mlp.fit(X_train_flat, y_train)

print("\n--- Evaluation ---")
y_pred = mlp.predict(X_test_flat)

print(f"Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%")
print(classification_report(y_test, y_pred, target_names=categories))

# Confusion Matrix Plot
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 5)) # Slightly larger figure
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=categories, yticklabels=categories)
plt.title('Confusion Matrix: Face Mask Detection')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

"""## Apply the XAI (LIME)"""

from skimage.segmentation import mark_boundaries

print("\n--- LIME Explanation ---")

# Initialize Explainer
explainer = lime_image.LimeImageExplainer()

# Select random image
idx = np.random.randint(0, len(X_test_norm))
img_to_explain = X_test_norm[idx]
print(f"Explaining image index: {idx}")

# LIME requires a function to predict. We use a lambda (anonymous function)
# to reshape the (N, 64, 64, 3) input from LIME back into (N, 12288) for the MLP
explanation = explainer.explain_instance(
    image=img_to_explain.astype('double'),
    classifier_fn=lambda x: mlp.predict_proba(x.reshape(x.shape[0], -1)),
    top_labels=1,
    hide_color=0,
    num_samples=1000
)

# Visualize
temp, mask = explanation.get_image_and_mask(
    explanation.top_labels[0],
    positive_only=True,
    num_features=5,
    hide_rest=False
)

plt.figure(figsize=(8, 4))
plt.subplot(1, 2, 1)
plt.imshow(img_to_explain)
plt.title("Original")
plt.axis('off')

plt.subplot(1, 2, 2)
plt.imshow(mark_boundaries(temp, mask))
plt.title("LIME Explanation")
plt.axis('off')
plt.show()